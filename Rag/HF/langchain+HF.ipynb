{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## huggingface_hub #api cqll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/flan-t5-small', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='google/flan-t5-small', client=<InferenceClient(model='google/flan-t5-small', timeout=120)>, async_client=<InferenceClient(model='google/flan-t5-small', timeout=120)>, task='text2text-generation')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/flan-t5-small\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,task=\"text2text-generation\",max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "# Use GPT-2 which works well on CPU\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"gpt2\",\n",
    "    task=\"text-generation\", \n",
    "    model_kwargs={\"max_length\":50},\n",
    "    temperature=0.7,# GPT-2 supports text generation\n",
    "    huggingfacehub_api_token=os.getenv(\"HF_TOKEN\") # Replace with your Hugging Face API token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamantiwari17/GenAI/Langchain/Basic+advance RAG/myenv1/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The world is at an extreme stage of technological innovation and AI is a rapidly emerging field. A recent study by the National Center for Science Education and Research at the University of California, Irvine (NCESR) and the American Institute of Child Health and Human Development (AIHHD) found that artificial intelligence (AI) is already more advanced than that of humans, with only about 100 percent of the world's population using the technology.\n",
      "\n",
      "Researchers have long predicted that AI could be the next stage of technology, and a new study from the National Academy of Sciences (NAS) suggests that AI could be the next major advancement in medicine and education, as well as the next major technological revolution.\n",
      "\n",
      "AI will not be the next big thing\n",
      "\n",
      "A new study by the National Academy of Sciences (NAS) and the University of California, Irvine (UCSI) suggests that AI is already more advanced than that of humans, with only about 100 percent of the world's population using the technology.\n",
      "\n",
      "Researchers at UCSI, the U.S. Department of Health and Human Services, and the University of California, Irvine (UCSI) recently released a new report titled \"An Important Threat to the Future of Human-Computer Interaction.\" They examined the world's most important technology developments, including computer vision, neural networks, and artificial intelligence.\n",
      "\n",
      "The report found that the future of human-computer interaction is likely to be a dynamic one, with a major shift in the way humans interact with technology.\n",
      "\n",
      "\"A key challenge is to be able to change the way we interact with technology, and that means making the most of the technology we have available to us,\" said Dr. Jeffrey Singleton, senior author of the report and the study's lead author. \"We are now at the point where we have to be extremely careful about how we use technology.\"\n",
      "\n",
      "The new report shows that AI is already at an extreme stage of technological innovation and AI is a rapidly emerging field.\n",
      "\n",
      "\"The world is at an extreme stage of technological innovation and AI is a rapidly emerging field,\" Singleton said. \"AI is already at an extreme stage of technological innovation, and we will continue to move toward the next stage of technology.\"\n",
      "\n",
      "The report also says that a key challenge is to be able to change the way we interact with technology, and that means making the most of the technology we have available to us.\n",
      "\n",
      "\"It's not just about creating a new kind of computing power, but it\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is AI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "# Use GPT-2 which works well on CPU\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"gpt2\",\n",
    "    task=\"text-generation\", \n",
    "    model_kwargs={\"max_length\":50},\n",
    "    temperature=0.7,# GPT-2 supports text generation\n",
    "    huggingfacehub_api_token=os.getenv(\"HF_TOKEN\") # Replace with your Hugging Face API token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamantiwari17/GenAI/Langchain/Basic+advance RAG/myenv1/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?)\\n\\nThere is no true MAchine LEarning, but the problem is that this is what the church is doing. If you know something about MAchine, you know that MAchine is a good way to get your income back. But if you don\\'t, you are \"too rich\" for it.\\n\\nIf you are rich, you are very likely to get MAchine LEarning. This is because the church is selling MAchine LEarning to the poor. This is called MAchine LEarning because the church is selling MAchine LEarning to the poor.\\n\\nIf you are poor, you are very likely to get MAchine LEarning. This is because the church is selling MAchine LEarning to the poor. This is called MAchine LEarning because the church is selling MAchine LEarning to the poor.\\n\\nYou can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning. You can\\'t just buy MAchine LEarning.\\n\\nIf you are poor, you are very likely to get MAchine LEarning. This is because the church is selling MAchine LEarning to the poor. This is called MAchine LEarning because the church is selling MAchine LEarning to the poor.\\n\\nIf you are poor, you are very likely to get MAchine LEarning. This is because the church is selling MAchine LEarning to the poor. This is called MAchine LEarning because the church is selling MAchine LEarning to the poor.\\n\\nIf you are poor, you are very likely to get MAchine LEarning. This is because the church is selling MAchine LEarning to the poor. This is called MAchine LEarning because the church is selling MAchine LEarning to the poor.\\n\\nIf you are poor, you are very likely to get MAchine LEarning. This is because the church is selling MAchine LEarning to the poor. This is called MAchine LEarning because the church is selling MAchine LEarning to the poor.\\n\\nIf'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is MAchine LEarning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "''' #try it some day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
