{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Working Flow OF RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##simple genai app using langchain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGSMITH_TRACING=\"true\"\n",
    "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT=\"pr-shadowy-jazz-89\"\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nSet up threads | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingTrace LangChain with OpenTelemetrySet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesSet up threadsOn this pageSet up threads\\nRecommended ReadingBefore diving into this content, it might be helpful to read the following:\\nAdd metadata and tags to traces\\n\\nMany LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\\nGroup traces into threads\\u200b\\nA Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\\nTo associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.\\nThe key value is the unique identifier for that conversation.\\nThe key name should be one of:\\n\\nsession_id\\nthread_id\\nconversation_id.\\n\\nThe value can be any string you want, but we recommend using UUIDs, such as f47ac10b-58cc-4372-a567-0e02b2c3d479.\\nCode example\\u200b\\nThis example demonstrates how to log and retrieve conversation history from LangSmith to maintain long-running chats.\\nYou can add metadata to your traces in LangSmith in a variety of ways, this code will show how to do so dynamically, but read the\\npreviously linked guide to learn about all the ways you can add thread identifier metadata to your traces.\\nPythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith import Clientimport langsmith as lsfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())langsmith_client = Client()# Config used for this examplelangsmith_project = \"project-with-threads\"session_id = \"thread-id-1\"langsmith_extra={\"project_name\": langsmith_project, \"metadata\":{\"session_id\": session_id}}# gets a history of all LLM calls in the thread to construct conversation historydef get_thread_history(thread_id: str, project_name: str): # Filter runs by the specific thread and projectfilter_string = f\\'and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"{thread_id}\"))\\' # Only grab the LLM runsruns = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type=\"llm\")]  # Sort by start time to get the most recent interaction  runs = sorted(runs, key=lambda run: run.start_time, reverse=True)  # The current state of the conversation  return runs[0].inputs[\\'messages\\'] + [runs[0].outputs[\\'choices\\'][0][\\'message\\']]# if an existing conversation is continued, this function looks up the current run’s metadata to get the session_id, calls get_thread_history, and appends the new user question before making a call to the chat model@traceable(name=\"Chat Bot\")def chat_pipeline(question: str, get_chat_history: bool = False): # Whether to continue an existing thread or start a new oneif get_chat_history:run_tree = ls.get_current_run_tree()messages = get_thread_history(run_tree.extra[\"metadata\"][\"session_id\"],run_tree.session_name) + [{\"role\": \"user\", \"content\": question}]else:messages = [{\"role\": \"user\", \"content\": question}]  # Invoke the model  chat_completion = client.chat.completions.create(      model=\"gpt-4o-mini\", messages=messages  )  return chat_completion.choices[0].message.content# Start the conversationchat_pipeline(\"Hi, my name is Bob\", langsmith_extra=langsmith_extra)import OpenAI from \"openai\";import { traceable, getCurrentRunTree } from \"langsmith/traceable\";import { Client } from \"langsmith\";import { wrapOpenAI } from \"langsmith/wrappers\";// Config used for this exampleconst langsmithProject = \"project-with-threads\";const threadId = \"thread-id-1\";const client = wrapOpenAI(new OpenAI(), {project_name: langsmithProject,metadata: { session_id: threadId }  });const langsmithClient = new Client();async function getThreadHistory(threadId: string, projectName: string) {// Filter runs by the specific thread and projectconst filterString = `and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"${threadId}\"))`;// Only grab the LLM runsconst runs = langsmithClient.listRuns({projectName: projectName,filter: filterString,runType: \"llm\"});// Sort by start time to get the most recent interactionconst runsArray = [];for await (const run of runs) {runsArray.push(run);}const sortedRuns = runsArray.sort((a, b) =>new Date(b.start_time).getTime() - new Date(a.start_time).getTime());// The current state of the conversationreturn [...sortedRuns[0].inputs.messages,sortedRuns[0].outputs.choices[0].message];}const chatPipeline = traceable(async (question: string,options: {getChatHistory?: boolean;} = {}) => {const {getChatHistory = false,} = options;  let messages = [];  // Whether to continue an existing thread or start a new one  if (getChatHistory) {    const runTree = await getCurrentRunTree();    const historicalMessages = await getThreadHistory(      runTree.extra.metadata.session_id,      runTree.project_name    );    messages = [      ...historicalMessages,      { role:\"user\", content: question }    ];  } else {    messages = [{ role:\"user\", content: question }];  }  // Invoke the model  const chatCompletion = await client.chat.completions.create({    model: \"gpt-4o-mini\",    messages: messages  });  return chatCompletion.choices[0].message.content;},{name: \"Chat Bot\",project_name: langsmithProject,metadata: { session_id: threadId }  });// Start the conversationawait chatPipeline(\"Hi, my name is Bob\");\\nAfter waiting a few seconds, you can make the following calls to contineu the conversation. By passing getChatHistory: true,\\nyou can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,\\ninstead of just responding to the latest message.\\nPythonTypeScript# Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)chat_pipeline(\"What is my name?\", get_chat_history=True, langsmith_extra=langsmith_extra)# Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)chat_pipeline(\"What was the first message I sent you\", get_chat_history=True, langsmith_extra=langsmith_extra)// Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)await chatPipeline(\"What is my name?\", { getChatHistory: true });// Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)await chatPipeline(\"What was the first message I sent you\", { getChatHistory: true });\\nView threads\\u200b\\nYou can view threads by clicking on the Threads tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.\\n\\nYou can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.\\n\\nYou can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTrace JS functions in serverless environmentsNextToubleshooting variable cachingGroup traces into threadsCode exampleView threadsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1 from data ingestion ->scrap data from Web\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/observability/how_to_guides/threads\")\n",
    "docs=loader.load()\n",
    "docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Set up threads | 🦜️🛠️ LangSmith\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingTrace LangChain with OpenTelemetrySet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesSet up threadsOn this pageSet up threads\n",
      "Recommended ReadingBefore diving into this content, it might be helpful to read the following:\n",
      "Add metadata and tags to traces\n",
      "\n",
      "Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\n",
      "Group traces into threads​\n",
      "A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\n",
      "To associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.\n",
      "The key value is the unique identifier for that conversation.\n",
      "The key name should be one of:\n",
      "\n",
      "session_id\n",
      "thread_id\n",
      "conversation_id.\n",
      "\n",
      "The value can be any string you want, but we recommend using UUIDs, such as f47ac10b-58cc-4372-a567-0e02b2c3d479.\n",
      "Code example​\n",
      "This example demonstrates how to log and retrieve conversation history from LangSmith to maintain long-running chats.\n",
      "You can add metadata to your traces in LangSmith in a variety of ways, this code will show how to do so dynamically, but read the\n",
      "previously linked guide to learn about all the ways you can add thread identifier metadata to your traces.\n",
      "PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith import Clientimport langsmith as lsfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())langsmith_client = Client()# Config used for this examplelangsmith_project = \"project-with-threads\"session_id = \"thread-id-1\"langsmith_extra={\"project_name\": langsmith_project, \"metadata\":{\"session_id\": session_id}}# gets a history of all LLM calls in the thread to construct conversation historydef get_thread_history(thread_id: str, project_name: str): # Filter runs by the specific thread and projectfilter_string = f'and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"{thread_id}\"))' # Only grab the LLM runsruns = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type=\"llm\")]  # Sort by start time to get the most recent interaction  runs = sorted(runs, key=lambda run: run.start_time, reverse=True)  # The current state of the conversation  return runs[0].inputs['messages'] + [runs[0].outputs['choices'][0]['message']]# if an existing conversation is continued, this function looks up the current run’s metadata to get the session_id, calls get_thread_history, and appends the new user question before making a call to the chat model@traceable(name=\"Chat Bot\")def chat_pipeline(question: str, get_chat_history: bool = False): # Whether to continue an existing thread or start a new oneif get_chat_history:run_tree = ls.get_current_run_tree()messages = get_thread_history(run_tree.extra[\"metadata\"][\"session_id\"],run_tree.session_name) + [{\"role\": \"user\", \"content\": question}]else:messages = [{\"role\": \"user\", \"content\": question}]  # Invoke the model  chat_completion = client.chat.completions.create(      model=\"gpt-4o-mini\", messages=messages  )  return chat_completion.choices[0].message.content# Start the conversationchat_pipeline(\"Hi, my name is Bob\", langsmith_extra=langsmith_extra)import OpenAI from \"openai\";import { traceable, getCurrentRunTree } from \"langsmith/traceable\";import { Client } from \"langsmith\";import { wrapOpenAI } from \"langsmith/wrappers\";// Config used for this exampleconst langsmithProject = \"project-with-threads\";const threadId = \"thread-id-1\";const client = wrapOpenAI(new OpenAI(), {project_name: langsmithProject,metadata: { session_id: threadId }  });const langsmithClient = new Client();async function getThreadHistory(threadId: string, projectName: string) {// Filter runs by the specific thread and projectconst filterString = `and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"${threadId}\"))`;// Only grab the LLM runsconst runs = langsmithClient.listRuns({projectName: projectName,filter: filterString,runType: \"llm\"});// Sort by start time to get the most recent interactionconst runsArray = [];for await (const run of runs) {runsArray.push(run);}const sortedRuns = runsArray.sort((a, b) =>new Date(b.start_time).getTime() - new Date(a.start_time).getTime());// The current state of the conversationreturn [...sortedRuns[0].inputs.messages,sortedRuns[0].outputs.choices[0].message];}const chatPipeline = traceable(async (question: string,options: {getChatHistory?: boolean;} = {}) => {const {getChatHistory = false,} = options;  let messages = [];  // Whether to continue an existing thread or start a new one  if (getChatHistory) {    const runTree = await getCurrentRunTree();    const historicalMessages = await getThreadHistory(      runTree.extra.metadata.session_id,      runTree.project_name    );    messages = [      ...historicalMessages,      { role:\"user\", content: question }    ];  } else {    messages = [{ role:\"user\", content: question }];  }  // Invoke the model  const chatCompletion = await client.chat.completions.create({    model: \"gpt-4o-mini\",    messages: messages  });  return chatCompletion.choices[0].message.content;},{name: \"Chat Bot\",project_name: langsmithProject,metadata: { session_id: threadId }  });// Start the conversationawait chatPipeline(\"Hi, my name is Bob\");\n",
      "After waiting a few seconds, you can make the following calls to contineu the conversation. By passing getChatHistory: true,\n",
      "you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,\n",
      "instead of just responding to the latest message.\n",
      "PythonTypeScript# Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)chat_pipeline(\"What is my name?\", get_chat_history=True, langsmith_extra=langsmith_extra)# Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)chat_pipeline(\"What was the first message I sent you\", get_chat_history=True, langsmith_extra=langsmith_extra)// Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)await chatPipeline(\"What is my name?\", { getChatHistory: true });// Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)await chatPipeline(\"What was the first message I sent you\", { getChatHistory: true });\n",
      "View threads​\n",
      "You can view threads by clicking on the Threads tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.\n",
      "\n",
      "You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.\n",
      "\n",
      "You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTrace JS functions in serverless environmentsNextToubleshooting variable cachingGroup traces into threadsCode exampleView threadsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\n",
      "\n",
      "' metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "for chunks in docs:\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Set up threads | 🦜️🛠️ LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APITrace with OpenAI Agents SDKCalculate token-based costs for'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='OpenAI Agents SDKCalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingTrace LangChain with OpenTelemetrySet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesSet up threadsOn this pageSet up threads'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Recommended ReadingBefore diving into this content, it might be helpful to read the following:\\nAdd metadata and tags to traces'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the Threads feature in LangSmith.\\nGroup traces into threads\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Group traces into threads\\u200b\\nA Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\\nTo associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='The key value is the unique identifier for that conversation.\\nThe key name should be one of:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='session_id\\nthread_id\\nconversation_id.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='The value can be any string you want, but we recommend using UUIDs, such as f47ac10b-58cc-4372-a567-0e02b2c3d479.\\nCode example\\u200b\\nThis example demonstrates how to log and retrieve conversation history from LangSmith to maintain long-running chats.\\nYou can add metadata to your traces in LangSmith in a variety of ways, this code will show how to do so dynamically, but read the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='previously linked guide to learn about all the ways you can add thread identifier metadata to your traces.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith import Clientimport langsmith as lsfrom langsmith.wrappers import wrap_openaiclient = wrap_openai(openai.Client())langsmith_client = Client()# Config used for this examplelangsmith_project = \"project-with-threads\"session_id = \"thread-id-1\"langsmith_extra={\"project_name\": langsmith_project, \"metadata\":{\"session_id\":'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='langsmith_project, \"metadata\":{\"session_id\": session_id}}# gets a history of all LLM calls in the thread to construct conversation historydef get_thread_history(thread_id: str, project_name: str): # Filter runs by the specific thread and projectfilter_string = f\\'and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"{thread_id}\"))\\' # Only grab the LLM runsruns ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='\"{thread_id}\"))\\' # Only grab the LLM runsruns = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type=\"llm\")]  # Sort by start time to get the most recent interaction  runs = sorted(runs, key=lambda run: run.start_time, reverse=True)  # The current state of the conversation  return runs[0].inputs[\\'messages\\'] + [runs[0].outputs[\\'choices\\'][0][\\'message\\']]#'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='+ [runs[0].outputs[\\'choices\\'][0][\\'message\\']]# if an existing conversation is continued, this function looks up the current run’s metadata to get the session_id, calls get_thread_history, and appends the new user question before making a call to the chat model@traceable(name=\"Chat Bot\")def chat_pipeline(question: str, get_chat_history: bool = False): # Whether to continue an existing thread or'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='# Whether to continue an existing thread or start a new oneif get_chat_history:run_tree = ls.get_current_run_tree()messages = get_thread_history(run_tree.extra[\"metadata\"][\"session_id\"],run_tree.session_name) + [{\"role\": \"user\", \"content\": question}]else:messages = [{\"role\": \"user\", \"content\": question}]  # Invoke the model  chat_completion = client.chat.completions.create('),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='= client.chat.completions.create(      model=\"gpt-4o-mini\", messages=messages  )  return chat_completion.choices[0].message.content# Start the conversationchat_pipeline(\"Hi, my name is Bob\", langsmith_extra=langsmith_extra)import OpenAI from \"openai\";import { traceable, getCurrentRunTree } from \"langsmith/traceable\";import { Client } from \"langsmith\";import { wrapOpenAI } from'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='} from \"langsmith\";import { wrapOpenAI } from \"langsmith/wrappers\";// Config used for this exampleconst langsmithProject = \"project-with-threads\";const threadId = \"thread-id-1\";const client = wrapOpenAI(new OpenAI(), {project_name: langsmithProject,metadata: { session_id: threadId }  });const langsmithClient = new Client();async function getThreadHistory(threadId: string, projectName: string) {//'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='string, projectName: string) {// Filter runs by the specific thread and projectconst filterString = `and(in(metadata_key, [\"session_id\",\"conversation_id\",\"thread_id\"]), eq(metadata_value, \"${threadId}\"))`;// Only grab the LLM runsconst runs = langsmithClient.listRuns({projectName: projectName,filter: filterString,runType: \"llm\"});// Sort by start time to get the most recent interactionconst'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='time to get the most recent interactionconst runsArray = [];for await (const run of runs) {runsArray.push(run);}const sortedRuns = runsArray.sort((a, b) =>new Date(b.start_time).getTime() - new Date(a.start_time).getTime());// The current state of the conversationreturn [...sortedRuns[0].inputs.messages,sortedRuns[0].outputs.choices[0].message];}const chatPipeline = traceable(async (question:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='chatPipeline = traceable(async (question: string,options: {getChatHistory?: boolean;} = {}) => {const {getChatHistory = false,} = options;  let messages = [];  // Whether to continue an existing thread or start a new one  if (getChatHistory) {    const runTree = await getCurrentRunTree();    const historicalMessages = await getThreadHistory(      runTree.extra.metadata.session_id,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='runTree.extra.metadata.session_id,      runTree.project_name    );    messages = [      ...historicalMessages,      { role:\"user\", content: question }    ];  } else {    messages = [{ role:\"user\", content: question }];  }  // Invoke the model  const chatCompletion = await client.chat.completions.create({    model: \"gpt-4o-mini\",    messages: messages  });  return'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='\"gpt-4o-mini\",    messages: messages  });  return chatCompletion.choices[0].message.content;},{name: \"Chat Bot\",project_name: langsmithProject,metadata: { session_id: threadId }  });// Start the conversationawait chatPipeline(\"Hi, my name is Bob\");'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='After waiting a few seconds, you can make the following calls to contineu the conversation. By passing getChatHistory: true,\\nyou can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,\\ninstead of just responding to the latest message.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='PythonTypeScript# Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)chat_pipeline(\"What is my name?\", get_chat_history=True, langsmith_extra=langsmith_extra)# Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)chat_pipeline(\"What was the first message I sent you\", get_chat_history=True,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='first message I sent you\", get_chat_history=True, langsmith_extra=langsmith_extra)// Continue the conversation (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE FRIST TRACE CAN BE INGESTED)await chatPipeline(\"What is my name?\", { getChatHistory: true });// Keep the conversation going (WAIT A FEW SECONDS BEFORE RUNNING THIS SO THE PREVIOUS TRACE CAN BE INGESTED)await chatPipeline(\"What was the first'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='INGESTED)await chatPipeline(\"What was the first message I sent you\", { getChatHistory: true });'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='View threads\\u200b\\nYou can view threads by clicking on the Threads tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='You can open up the trace or annotate the trace in a side panel by clicking on Annotate and Open trace, respectively.Was this page helpful?You can leave detailed feedback on GitHub.PreviousTrace JS functions in serverless environmentsNextToubleshooting variable cachingGroup traces into threadsCode exampleView threadsCommunityDiscordTwitterGitHubDocs CodeLangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###load data-->Docs--->divide into tunks--->text-->vectors-->vectorembedding-->vector\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=400,chunk_overlap=50)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeedings=HuggingFaceEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x769f08140320>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeedings)\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Group traces into threads\\u200b\\nA Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\\nTo associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"A Thread is a sequence of traces representing a single conversation\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "#from langchain_openai import ChatOpenAI-same as gemini\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=' \\n      Answet the following question based only on the provided text:\\n      <context>\\n      {context}\\n      </context>\\n      Question: {input}\\n      Provide a detailed explanation in your answer.\\n    '), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x769f08142030>, default_metadata=())\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##retevial chain->document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "      Answet the following question based only on the provided text:\n",
    "      <context>\n",
    "      {context}\n",
    "      </context>\n",
    "      Question: {input}\n",
    "      Provide a detailed explanation in your answer.\n",
    "    \"\"\"\n",
    ")\n",
    "doc_chain=create_stuff_documents_chain(llm,prompt)\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That is correct. The provided text states: \"A Thread is a sequence of traces representing a single conversation.\"'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##just a test part\n",
    "from langchain_core.documents import Document\n",
    "doc_chain.invoke(\n",
    "    {\n",
    "        \"input\":\"A Thread is a sequence of traces representing a single conversation\",\n",
    "        \"context\":[Document(page_content=\"A Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread\")],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x769f08140320>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=' \\n      Answet the following question based only on the provided text:\\n      <context>\\n      {context}\\n      </context>\\n      Question: {input}\\n      Provide a detailed explanation in your answer.\\n    '), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x769f08142030>, default_metadata=())\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## input-->Reterver-->vector store databse\n",
    "retirver=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "rt_chain=create_retrieval_chain(retirver,doc_chain)#context information get from doc_chain and VDB from retiever\n",
    "rt_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Thread is defined as a sequence of traces. These traces are linked together because they are part of the same conversation. Each response in the conversation is represented as an individual trace within the thread.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##get the response from The LLM\n",
    "response=rt_chain.invoke({\"input\":\"A Thread is a sequence of traces representing a single conversation\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'A Thread is a sequence of traces representing a single conversation',\n",
       " 'context': [Document(id='e30f838b-c32e-4958-aec4-f5fd92e2abbf', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Group traces into threads\\u200b\\nA Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\\nTo associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.'),\n",
       "  Document(id='73ddcee5-0222-4a38-a5dc-7b772cad13a7', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='session_id\\nthread_id\\nconversation_id.'),\n",
       "  Document(id='3227cd1b-8532-4644-ac66-b91190106e43', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.'),\n",
       "  Document(id='f468fc09-3a8a-47de-bfd5-520f53781900', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='\"{thread_id}\"))\\' # Only grab the LLM runsruns = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type=\"llm\")]  # Sort by start time to get the most recent interaction  runs = sorted(runs, key=lambda run: run.start_time, reverse=True)  # The current state of the conversation  return runs[0].inputs[\\'messages\\'] + [runs[0].outputs[\\'choices\\'][0][\\'message\\']]#')],\n",
       " 'answer': 'A Thread is defined as a sequence of traces. These traces are linked together because they are part of the same conversation. Each response in the conversation is represented as an individual trace within the thread.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e30f838b-c32e-4958-aec4-f5fd92e2abbf', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='Group traces into threads\\u200b\\nA Thread is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\\nTo associate traces together, you need to pass in a special metadata key where the value is the unique identifier for that thread.'),\n",
       " Document(id='73ddcee5-0222-4a38-a5dc-7b772cad13a7', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='session_id\\nthread_id\\nconversation_id.'),\n",
       " Document(id='3227cd1b-8532-4644-ac66-b91190106e43', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='You can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.'),\n",
       " Document(id='f468fc09-3a8a-47de-bfd5-520f53781900', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/threads', 'title': 'Set up threads | 🦜️🛠️ LangSmith', 'description': 'Before diving into this content, it might be helpful to read the following:', 'language': 'en'}, page_content='\"{thread_id}\"))\\' # Only grab the LLM runsruns = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type=\"llm\")]  # Sort by start time to get the most recent interaction  runs = sorted(runs, key=lambda run: run.start_time, reverse=True)  # The current state of the conversation  return runs[0].inputs[\\'messages\\'] + [runs[0].outputs[\\'choices\\'][0][\\'message\\']]#')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']#you will know the context has taken"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
